# Advanced Analytics with Spark - Predicting Forest Cover
Created by Sean Owen + Nisha Muktewar (sowen@cloudera.com, nisha@cloudera.com)<br>
This project demonstrates using random forests to predict forest cover using Apache Spark and Spark MLLib

<b>Status</b>: Demo Ready<br>
<b>Use Case</b>: Predicting Forest Cover

<b>Steps</b>:<br>
1. In the project go to Setting > Environment > Spark Configuration and put in AAS-predicting-forest-cover/spark-defaults.conf, then save. 
2. Open a terminal and run setup.sh<br>
3. Create a scala Session and run CH04-random-forest.scala<br>
4. When finished, run cleanup.sh in the terminal<br>

<b>Recommended Session Sizes</b>: 4 CPU, 4 GB RAM

<b>Estimated Runtime</b>: <br>
CH05-kmeans.py --> approx 15-30 min 

<b>Recommended Jobs/Pipeline</b>:<br>
None

<b>Demo Script</b><br>
TBD

<b>Related Content</b>:<br>
https://github.com/sryza/aas

